 L02 — Traditional Graph Machine Learning

 1. Random Walk as a Stochastic Process

A random walk on a graph defines a Markov chain over nodes.

Transition probability:
\[
P(u \rightarrow v) = \frac{1}{\deg(u)}
\]

Key property:
 The next state depends only on the current node (Markov property).
 The longrun distribution may converge to a stationary distribution.

Interpretation:
Graph structure defines a probability flow over nodes.



 2. Stationary Distribution and Centrality

The stationary distribution π satisfies:

\[
\pi = \pi P
\]

This gives a structural notion of importance:
Nodes visited more often in the long run are more "central".

This shifts graph thinking from:
 combinatorial structure
to
 probabilistic dynamics.



 3. PageRank = Random Walk with Teleportation

PageRank modifies the random walk:

\[
\pi = \alpha P^T \pi + (1  \alpha) \frac{1}{N}
\]

Where:
 α = damping factor (typically 0.85)
 Teleportation ensures irreducibility and convergence.

Key insight:
Graph importance is defined by recursive endorsement.

Node importance depends on important neighbors.



 4. Centrality Measures as Structural Signals

Different centralities capture different notions of influence:

 Degree centrality → local connectivity
 Betweenness centrality → structural bottleneck
 PageRank → global recursive importance

These are feature extractors over graph topology.

Traditional graph ML pipelines:
1. Compute structural features
2. Feed into classifier or ranking model



 5. Bridge to Representation Learning

Limitations of traditional graph ML:

 Handcrafted structural features
 Taskspecific engineering
 Limited expressiveness

Key transition:
Instead of designing centrality features manually,
learn node embeddings via neighborhood aggregation.

Random walk → Node sequences  
Sequences → Cooccurrence  
Cooccurrence → Embedding  

This leads directly to:
DeepWalk / node2vec (L03).



 Meta Insight

Traditional graph ML views graph as:

 A matrix (adjacency / transition matrix)
 A stochastic process (random walk)
 A ranking system (centrality)

Graph neural networks will reinterpret this as:

Message Passing = Learnable neighborhood aggregation.
